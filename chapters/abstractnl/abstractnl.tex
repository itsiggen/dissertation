% !TeX root = ../../thesis.tex
\chapter{Beknopte samenvatting}

De automatisering en productiviteitswinsten die door de nieuwste ontwikkelingen in AI worden gebracht, hebben de adoptie ervan versneld in diverse domeinen, van nieuws, bestuur en financiën tot gezondheidszorg, reclame, werk en onderwijs. Terwijl we overstappen naar geautomatiseerde besluitvorming die vaak ondoorzichtiger en minder verantwoordelijk is, zijn er aanzienlijke kortetermijnrisico's, terwijl de langetermijngevolgen nog moeten blijken. Naast adversariële voorbeelden, de oorspronkelijke faalmodus van AI, moeten we nu omgaan met bredere uitdagingen, direct of indirect veroorzaakt door AI. Deze uitdagingen omvatten privacy, intellectuele rechten, surveillance, malware, phishing, desinformatiecampagnes en allerlei vormen van bevooroordeelde of onbetrouwbare informatie. Het is geen nieuwe observatie dat, terwijl wij onze tools vormgeven, onze tools ons ook beïnvloeden, maar in deze tijd hebben deze tools ongekende toegang tot data en rekenkracht.

Ook het domain van cybersecurity plukt de vruchten van AI, in zowel offensieve als defensieve toepassingen. De integratie van AI-gebaseerde oplossingen vergroot echter het aanvalsoppervlak en verandert het dreigingslandschap aanzienlijk. In dit proefschrift onderzoeken we adaptieve aanvallen tegen AI-systemen, en hoe we deze systemen robuust kunnen laten functioneren onder adversariële aanvallen. We introduceren eerst een nieuwe aanval op een realistisch systeem, Google reCAPTCHA v3, een Captcha die het browsen niet onderbreekt en de aard van de uitdaging verbergt: de Turingtest. We ontwikkelen verschillende agenten die mensachtig webgedrag simuleren en trainen ze om detectie te ontwijken door de risicoscore als informatief signaal te gebruiken, op een online en interactieve manier. We tonen aan hoe deze agenten, met algemene webnavigatievaardigheden, detectie bijna perfect kunnen ontwijken op andere websites. Zo onthullen we een kwetsbaarheid: toegang tot de risicoscore stelt aanvallers in staat om betere ontwijkende agenten te ontwikkelen.

Daarnaast is het bekend dat AI-modellen kwetsbaar zijn voor ontwijkingsaanvallen en valse correlaties, en dat geldt ook voor malwaredetectie. In samenwerking met andere universiteiten en industriële partners onderzoeken we de machine learning-gebaseerde detectiemodule van een bekende antivirus, om deze te versterken tegen kwaadaardige malware. Adversariële training, de meest betrouwbare verdediging tegen ontwijking, is hier niet direct toepasbaar, omdat er geen consistente manier is om aanpassingen in de programmaruimte aan te brengen. We introduceren een nieuwe aanpak met reinforcement learning om adversariële voorbeelden te maken, die twee voordelen heeft: het voert alleen haalbare aanpassingen uit in de programmaruimte, en het biedt theoretische garanties voor de robuustheid van het model tegen een specifieke set aanvallers. In onze evaluatie kunnen we het aanvankelijke succespercentage van aanvallen consequent tot nul terugbrengen na enkele retraining-iteraties.

Tot slot blijven AI-systemen in de praktijk kwetsbaar voor besluitvormingsaanvallen, ondanks aanzienlijke inspanningen om ze robuuster te maken, omdat definitieve bewijzen van hun robuustheid moeilijk te verkrijgen zijn. We introduceren een framework voor het optimaliseren van black-box aanvallen en verdedigingen door de competitieve dynamiek die ze vormen. Het is essentieel om realistische en worst-case aanvallen te evalueren om robuustheid te meten. We sturen de aanvallen en hun ontwijkingsmogelijkheden adaptief aan, en doen hetzelfde voor de verdedigingen. We tonen aan dat actieve verdedigingen, die het gedrag van het systeem controleren, een noodzakelijke aanvulling zijn op modelversterking bij besluitvormingsaanvallen; vervolgens hoe deze verdedigingen door adaptieve aanvallen kunnen worden omzeild, om uiteindelijk actieve en adaptieve verdedigingen te versterken. Onze bevindingen, ondersteund door theoretisch en empirisch onderzoek, bevestigen dat AI-ondersteunde aanvallers een grote bedreiging vormen voor black-box modellen, wat de wapenwedloop opnieuw aanwakkert, waarbij verdedigingen ook door AI moeten worden ondersteund.

Samenvattend leveren we bijdragen aan de beveiliging van AI-systemen door adaptieve aanvallen en verdedigingen te onderzoeken in verschillende domeinen. Onze werken bouwen op elkaar voort en bieden bij hun synthese belangrijke inzichten en een technisch overzicht van black-box aanvallen op machine learning, en leggen de basis voor verder onderzoek naar veilige, robuuste en betrouwbare AI-systemen.

\instructionsabstract

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
