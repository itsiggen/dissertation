\chapter{Background}\label{ch:background}

This dissertation, as well as all the works that it encompasses, are built on top of two fundamental pillars: \gls{AML} and \gls{RL}.
The purpose of this chapter is to introduce to the reader fundamental notions from these two scientific domains, with a focus on those that are relevant to this dissertation and the ease of its understanding.

\section{Adversarial Machine Learning}
\gls{AML} is a field of study within machine learning \gls{ML} that focuses on the vulnerabilities of \gls{ML} models to adversarial attacks.
This field gained prominence in the early 2010s when researchers discovered that small, often imperceptible perturbations to input data could cause significant errors in ML model predictions \cite{biggio2013evasion, szegedy2013intriguing}.
These findings highlighted critical security issues in ML systems, that subsequently sparked a burgeoning scientific field and extensive research into understanding, exploiting, and ultimately defending against these vulnerabilities.

After their inception, adversarial attacks on ML models enjoyed a prolific growth in the scientific community, where researchers applied them anywhere and everywhere they could.
To provide a general view to adversarial attacks, they can be categorized into several types according and along two axes: the level of access they have to the model, as well as the objective of the attack.

\subsection{Adversarial Objective}

While not intended as an exhaustive list, adversarial attacks fall largely in four separate categories:

\begin{itemize}
    \item \textbf{Evasion Attacks}. These are attacks that occur at test time. An adversary slightly modifies input data to ``evade'' detection or classification. For instance, adding noise to an image can mislead an image classifier into making an incorrect prediction.
    \item \textbf{Poisoning Attacks}. Contrary to evasion, poisoning attacks occur at training time. The attacker injects malicious data into the training set, aiming to corrupt the learning process. This aim can be the degradation of the model's performance or the insertion of backdoors (triggers) that enable a special kind of evasion at test time.
    \item \textbf{Model Extraction Attacks}. In model extraction attacks, the adversary aims to steal the parameters or hyperparameters of a model. By querying the model and analyzing the outputs, an attacker can approximate the original model.
    \item \textbf{Membership Inference Attacks} These attacks involve determining whether a specific data point was part of the model's training set. This can lead to privacy breaches, revealing sensitive information about individuals in the dataset.
\end{itemize}

\textbf{Adversarial Examples.} The most common attack in \gls{AML} is evasion, and the samples that are part of such an attack or the results of it, are known as adversarial examples.
Adversarial examples are inputs to \gls{ML} models that are intentionally designed to cause the model to make a mistake.
These inputs are typically created by adding small, carefully crafted perturbations to legitimate examples from the dataset; these modifications are almost imperceptible to humans but cause significant errors in the model predictions.
The study of adversarial examples has revealed vulnerabilities in various machine learning models, particularly in deep neural networks, raising concerns about their robustness and security in real-world applications.


Formally, let \( f: \mathbb{R}^n \to \mathbb{R}^m \) be a machine learning model, where \( f(x) \) represents the model's output for an input \( x \in \mathbb{R}^n \). Consider a legitimate input \( x \) with true label \( y \). An adversarial example \( x' \) is a perturbed version of \( x \) such that:

\begin{enumerate}
    \item \( x' = x + \delta \), where \( \delta \in \mathbb{R}^n \) is the perturbation. 
    \item The perturbation \( \delta \) is constrained to be smaller than a threshold \(\epsilon\), often measured by a norm \( \|\delta\|_p \leq \epsilon\) for some norm \( p \); common norm choices are \( p = 2 \) (Euclidean) or \( p = \infty \) (maximum).
    \item The perturbed input \( x' \) leads to a misclassification by the model, i.e., \( f(x') \neq y \) or, more generally, \( f(x') \neq f(x) \).
\end{enumerate}

In practice, adversarial examples are often generated by optimizing the perturbation \( \delta \) to maximize the  prediction error of the model, subject to the  perturbation constraint.
While adversarial examples were initially discovered and subsequently researched in the image classification domain, eventually they appeared in every domain that employs ML-based classification.
Naturally, different domains will have different constraints; in adversarial malware for example, the similarity to the original sample is of less importance, while the preservation of semantics and functionality is of high importance. 

\subsubsection{White-Box vs. Black-Box}

In white-box (evasion) attacks, the attacker has complete access to the model, including its architecture, parameters, and gradients.
This allows for the precise calculation of perturbations that can mislead the model.
Formally, white-box attacks often solve the following optimization problem:
\[
\max_{\delta} \mathcal{L}(f(x + \delta), y)
\]
subject to
\[
\|\delta\|_p \leq \epsilon,
\]
where $\mathcal{L}$ is the loss function, $f$ is the model, $x$ is the original input, $y$ is the true label, $\|\cdot\|_p$ is the $p$-norm, and $\epsilon$ is the small constant that bounds the perturbation.
Common methods for white-box attacks include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).

In contrast, in black-box adversarial attacks, the adversary does not have access to the internals of the model, a much more representative formalism for cybersecurity enviroments.
Instead, the one capability that is typically preserved for the adversary in black-box contexts, is the ability to query the model and observe the output.

The lack of access to the underlying model and lack of a closed-form description of it, makes it challenging to compute gradients directly.
Alternative strategies need to be employed for optimizing the attack: such approaches include the approximation of the gradient with techniques like finite differences or surrogate models, or derivative free optimization like genetic algorithms.

\subsection{Mitigations and Defenses}

Several defense mechanisms or mitigations to adversarial examples have been proposed and tested over time:

\begin{itemize}
    \item \textbf{Adversarial Training}. This defense incorporates adversarial examples into the training process to enhance model robustness. A computationally intensive approach but with significant efficacy.
    \item \textbf{Defensive Distillation}. This technique involves training a model to output probabilities of class labels, which can help in smoothing the decision boundary and making the model less sensitive to adversarial perturbations.
    \item \textbf{Gradient Masking}. Hiding or obfuscating the gradient information makes it harder for attackers to craft adversarial examples. However, this approach has been shown to be often ineffective against stronger attacks.
    \item \textbf{Ensemble Methods}. Using multiple models and aggregating their predictions can provide a level of robustness, as it is less likely that all models will be equally vulnerable to the same adversarial examples.
    \item \textbf{Feature Squeezing / Denoising}. Reducing the complexity of the input space (e.g., by bit-depth reduction) or by using denoising methods, a defense has the potential to remove adversarial perturbations.
\end{itemize}

We should mention that all these defenses confer only empirical robustness to a model; so far there have been no definitive proofs of a model's operational robustness at deployment.
Notably, the only defense that has withstood the test of time and is still widely used is adversarial training.
Adversarial training can be mathematically formulated as a min-max optimization problem:

\begin{equation}
\min_{\theta} \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \max_{\delta \in \Delta} L(f_\theta(x + \delta), y) \right],
\end{equation}

where \( \theta \) represents the model parameters, \( \mathcal{D} \) is the data distribution, \( \Delta \) is the set of permissible perturbations, \( L \) is the loss function, and \( f_\theta \) is the model.
The inner maximization seeks the worst-case perturbation \( \delta \), while the outer minimization adjusts the model parameters \( \theta \) to minimize the loss; as essentially a saddle-point optimization task.
As both the loss landscape and the set of perturbations are typically non-convex, adversarial training can be a challenging and computationally intensive task; even leading to instability and convergence issues due to the alternating nature of the optimization.

\subsection{Shortcut Learning}

Shortcut learning refers to models leveraging simple but non-generalizable patterns in the training data to make predictions \cite{geirhos2020shortcut}.
These shortcuts can  make models vulnerable to adversarial attacks and lead to poor performance on out-of-distribution samples; the later should be considered as given in adversarial environments like cybersecurity.
Adversarial examples can exploit these shortcuts by introducing small perturbations that the model erroneously relies on for prediction.

When \gls{ML} models are trained without adversarial training or without considering robustness to attacks, then they are more likely to depend on any shortcuts or artifacts present in the data, and even be more vulnerable to attacks.
As a result, these models will perform poorly when exposed to adversarial examples and other forms of distributional shifts, limiting their real-world applicability and reliability.
It is thus paramount to identify any reliance on non-robust features beforehand and mitigate them.x

\section{Reinforcement Learning}

\gls{RL} is a paradigm of \gls{ML} where an agent learns to make decisions by interacting with an environment.
The agent takes actions in an environment, receives feedback in the form of rewards, and adjusts its actions to maximize cumulative rewards over time.
This trial-and-error approach allows the agent to discover optimal behaviors through experience rather than by being explicitly programmed.

The foundational concept in \gls{RL} is the \gls{MDP}, which provides a mathematical framework for modeling the decision-making environment.
An MDP consists of states, actions, rewards, and state transition probabilities, and it assumes the Markov property where the future state depends only on the current state and action.

\gls{RL} differs significantly from other \gls{ML} paradigms such as supervised learning and unsupervised learning.
In supervised learning, an algorithm learns from a labeled dataset, and in unsupervised learning, it discovers patterns in data without explicit labels.
\gls{RL}, however, deals with learning from the consequences of actions, making it suitable for problems where an agent must make a sequence of decisions.

Applications of \gls{RL} span across diverse fields including robotics, where robots learn to navigate and manipulate objects; games, where agents learn to play and excel at from chess to Go and video games; finance, where trading strategies are optimized; and as it most prominently featured in this dissertation, cybersecurity, where agents learn how to attack or defend AI-based systems.
The versatility and generality of \gls{RL} have made it a focal area of research as we collecticely accelerate towards artificial general intelligence.

Despite its promise, \gls{RL} poses several challenges that oft make it a bad choice to apply it uncritically and out-of-the-box.
Such challenges are the exploration-exploitation trade-off, where an agent must balance exploring the environment to discover new strategies and exploiting known strategies to maximize rewards; sparse or delayed rewards, where an agent following a suboptimal initial policy might never reach them; and others like the design of reward functions, the scalability to high-dimensional state and action spaces, and ensuring stability and convergence of learning algorithms, which all are ongoing research areas.

In this chapter, we will delve into the theoretical underpinnings of \gls{RL}, explore various \gls{RL} algorithms, and examine their applications and challenges.
We will also discuss topics such as function approximation with neural networks, multi-agent reinforcement learning, and the implications of \gls{RL} for artificial general intelligence and AI safety.

\section{Markov Decision Processes}

Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making problems where outcomes are partly random and partly under the control of a decision-maker. MDPs are widely used in reinforcement learning to formalize the environment in which an agent operates.

An MDP is defined by the tuple $(S, A, P, R, \gamma)$, where:
\begin{itemize}
    \item $S$ is a finite set of states.
    \item $A$ is a finite set of actions.
    \item $P: S \times A \times S \rightarrow [0,1]$ is the state transition probability function, where $P(s'|s,a)$ denotes the probability of transitioning to state $s'$ from state $s$ after taking action $a$.
    \item $R: S \times A \rightarrow \mathbb{R}$ is the reward function, where $R(s,a)$ denotes the immediate reward received after taking action $a$ in state $s$.
    \item $\gamma \in [0,1]$ is the discount factor, which determines the importance of future rewards.
\end{itemize}

The objective in an MDP is to find a policy $\pi: S \rightarrow A$ that maximizes the expected cumulative reward, also known as the return. The return $G_t$ at time $t$ is defined as:
\[
G_t = \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k})
\]
where $s_{t+k}$ and $a_{t+k}$ are the states and actions at time $t+k$, respectively.

\subsection{Value Functions}

Value functions are used to estimate the expected return from a given state or state-action pair under a specific policy. The state-value function $V^{\pi}(s)$ for policy $\pi$ is defined as the expected return when starting in state $s$ and following policy $\pi$ thereafter:
\[
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s \right]
\]

The action-value function $Q^{\pi}(s, a)$ for policy $\pi$ is defined as the expected return when starting in state $s$, taking action $a$, and then following policy $\pi$ thereafter:
\[
Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s, a_t = a \right]
\]

The goal of reinforcement learning is often to find the optimal policy $\pi^*$ that maximizes the value function:
\[
\pi^* = \arg\max_{\pi} V^{\pi}(s) \quad \forall s \in S
\]

\textbf{Advantage Functions.} The advantage function $\hat{A}_t$ is sometimes used in place of a value function, , as it is crucial for reducing the variance in policy gradient methods for example.
It can be estimated in various ways, such as in Generalized Advantage Estimation (GAE), which provides a balance between bias and variance:
\[
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\]
where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the temporal difference error, and $\lambda$ is a hyperparameter that controls the trade-off.

\subsection{Bellman Equations}

The Bellman equations provide recursive definitions for the value functions. For the state-value function, the Bellman equation is:
\[
V^{\pi}(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \left[ R(s,a) + \gamma V^{\pi}(s') \right]
\]

For the action-value function, the Bellman equation is:
\[
Q^{\pi}(s, a) = R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) \sum_{a' \in A} \pi(a'|s') Q^{\pi}(s', a')
\]

The optimal state-value function $V^*(s)$ and the optimal action-value function $Q^*(s, a)$ satisfy the Bellman optimality equations:
\[
V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \left[ R(s,a) + \gamma V^*(s') \right]
\]

\[
Q^*(s, a) = R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) \max_{a' \in A} Q^*(s', a')
\]

The two fundamental approaches for solving MDPs are called policy iteration and value iteration.

\subsubsection{Policy Iteration}

Policy iteration alternates between policy evaluation and policy improvement:
\begin{itemize}
    \item \textbf{Policy Evaluation}: Calculate the value function $V^{\pi}$ for the current policy $\pi$ by solving the Bellman equation.
    \item \textbf{Policy Improvement}: Update the policy $\pi$ by choosing actions that maximize the value function.
\end{itemize}

The algorithm converges to the optimal policy $\pi^*$.

\subsubsection{Value Iteration}

Value iteration simplifies policy iteration by combining policy evaluation and policy improvement into a single step.
It iteratively updates the value function using the Bellman optimality equation:
\[
V_{k+1}(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \left[ R(s,a) + \gamma V_k(s') \right]
\]

The algorithm converges to the optimal value function $V^*$, and the optimal policy $\pi^*$ can be derived from $V^*$.


\section{RL Algorithms}

As \gls{MDP}s can be solved in various ways, it is important to understand how different \gls{RL} algorithms achieve that.
These algorithms can be broadly categorized based on two criteria.
One such distinction is between model-free and model-based methods, while another is between on-policy and off-policy methods. Understanding these categories helps in selecting the appropriate algorithm for a given problem and environment.

\subsubsection{Model-Free vs Model-Based}

Model-free are algorithms that do not attempt to learn a model of the environment or its dynamics; instead, they focus directly on learning the policy or the value function.
While model-free algorithms are generally simpler and require fewer assumptions, they might need more interactions with the environment to learn effective policies.
The vast majority of applied RL uses model-free methods, further subdivided to value-based methods like Q-learning and SARSA, and policy-based methods which directly learn the policy, such as REINFORCE and actor-critic.

Model-based algorithms, on the other hand, attempt to learn a model of the dynamics of the environment, which includes the state transition probabilities and the reward function.
These models are then used for planning and decision-making.
Model-based algorithms can be more powerful and data-efficient but also more complicated as they require a model of the environment; model-free algorithms are typically simpler and more straightforward to implement.

\subsubsection{On-Policy vs Off-Policy Algorithms}

On-policy algorithms learn the value of the policy that is currently being used to make decisions. 
The policy is evaluated and improved continually, based on the actions taken by the agent.

Off-policy algorithms, in contrast, learn the value of the optimal policy independently of the agent actions.
This allows off-policy methods to reuse experience generated from older policies, at the cost of less accurate updates.
In some off-policy algorithms, importance sampling is used to correct the bias introduced by using experiences from a different policy.
% \[
% \hat{Q}(s, a) = \frac{1}{N} \sum_{i=1}^{N} \rho_i \delta_i
% \]
% where $\rho_i$ is the importance sampling ratio and $\delta_i$ is the temporal-difference error.

A (non-exhaustive) list of commonly used algorithms in RL follows:

\begin{itemize}
    \item \textbf{Q-learning}: A popular model-free and off-policy algorithm that learns the action-value function $Q(s, a)$.
    \[
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
    \]
    \item \textbf{Deep Q-Networks (DQN)}: An extension of Q-learning that uses deep neural networks to approximate the action-value function. It leverages experience replay and target networks to stabilize training.
    \[
    Q(s, a; \theta) \approx Q^*(s, a)
    \]
    \item \textbf{SARSA (State-Action-Reward-State-Action)}: An on-policy algorithm that updates the action-value function based on the action taken by the policy.
    \[
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
    \]
    \item \textbf{REINFORCE}: A Monte Carlo policy gradient algorithm that updates the policy parameters $\theta$.
    \[
    \theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) G_t
    \]
    \item \textbf{Actor-Critic}: Combines value-based and policy-based methods by having two components: an actor that updates the policy and a critic that evaluates the policy.
    \[
    \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
    \]
    \[
    \theta \leftarrow \theta + \alpha \delta_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
    \]
    \[
    w \leftarrow w + \beta \delta_t \nabla_w V(s_t)
    \]
    \item \textbf{Dyna-Q}: A model-based hybrid algorithm that integrates learning, planning, and acting. It learns the model of the environment and uses it to simulate experiences.
    \[
    P(s'|s,a) \leftarrow \text{update model from real experience}
    \]
    \[
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
    \]
    \item \textbf{AlphaZero}: A state-of-the-art model-based algorithm that uses Monte Carlo Tree Search (MCTS) for planning and neural networks to represent the policy and value functions. It learns from self-play by iteratively improving the model.
\end{itemize}

\section{Proximal Policy Optimization}

\gls{PPO} is a state-of-the-art \gls{RL} algorithm that combines the stability of trust region methods with the scalability of policy gradient methods.
Trust regions methods, like Trust Region Policy Optimization (TRPO), ensure that the policy updates are not too large by imposing a Kullback-Leibler (KL) divergence constraint, 
Policy gradient methods like REINFORCE, make policy updates are proportional to the gradient of the log probability of the actions taken, weighted by the rewards obtained.
For stable and reliable policy updates, \gls{PPO} employs either a clipped surrogate objective (PPO-clip) or a KL-penalty objective (PPO-penalty).

PPO-clip uses a clipped surrogate objective to ensure stability, and is written as:
\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
\]
where:
\begin{itemize}
    \item $r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ is the probability ratio between the new policy $\pi_{\theta}$ and the old policy $\pi_{\theta_{\text{old}}}$.
    \item $\hat{A}_t$ is the advantage function, which estimates the relative value of action $a_t$ compared to the average action at state $s_t$.
    \item $\epsilon$ is a hyperparameter that controls the size of the trust region.
\end{itemize}

The clipped objective ensures that the policy updates are constrained, thereby maintaining stability and preventing drastic changes that could degrade performance.
Instead, PPO-penalty promotes smoother updates by penalizing large deviations from the old policy, and introduces a KL divergence penalty to the objective function, which is written as:
\[
L^{\text{KL}}(\theta) = \mathbb{E}_t \left[ r_t(\theta) \hat{A}_t - \beta \text{KL}[\pi_{\theta_{\text{old}}}(\cdot|s_t) \| \pi_{\theta}(\cdot|s_t)] \right]
\]
where:
\begin{itemize}
    \item $\text{KL}[\pi_{\theta_{\text{old}}}(\cdot|s_t) \| \pi_{\theta}(\cdot|s_t)]$ is the KL divergence between the old policy and the new policy.
    \item $\beta$ is a coefficient that controls the importance of the KL divergence term.
\end{itemize}

Both PPO-clip and PPO-penalty alternate between sampling data through interaction with the environment and optimizing the surrogate objective using stochastic gradient descent.
The steps are shown in Algorithm \ref{alg:PPOgen}.

\begin{algorithm}
\caption{Proximal Policy Optimization (PPO)}
\label{alg:PPOgen}
\begin{algorithmic}[1]
\STATE Initialize policy parameters $\theta_0$ and value function parameters $\phi_0$.
\FOR{each iteration}
    \STATE Collect a set of trajectories $\{(s_t, a_t, r_t, s_{t+1})\}$ by running the current policy $\pi_{\theta_k}$ in the environment.
    \STATE Compute advantage estimates $\hat{A}_t$ based on the collected trajectories.
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \STATE Optimize the PPO objective $L^{\text{CLIP}}(\theta)$ or $L^{\text{KL}}(\theta)$ with respect to $\theta$ using the collected data, to update the policy parameters:
    \[
    \theta_{k+1} = \theta_k + \alpha \nabla_\theta L(\theta)
    \]
    \STATE Update the value function by minimizing the mean-squared error loss:
    \[
    L^{\text{VF}}(\phi) = \mathbb{E}_t \left[ (V_{\phi}(s_t) - R_t)^2 \right]
    \]
\ENDFOR
\end{algorithmic}
\end{algorithm}

In summary, \gls{PPO} is a method that combines the advantages of policy gradients with stability enhancements through its clipped or penalized objective function.
The use of neural networks for function approximation enables it to handle complex, high-dimensional environments, making it a powerful \gls{RL} tool for modern cybersecurity environments.
\gls{PPO} is prominently featured in all most of the works in this dissertation.

\subsection{Function Approximation with Neural Networks}

In complex environments with large or continuous state and action spaces, traditional tabular methods for representing value functions or policies become intractable.
Neural networks provide a powerful tool for approximating these functions, enabling RL algorithms to scale to high-dimensional problems.
Different architectures of neural networks can be used depending on the characteristics of the environment and the task.
Such architectures include:

\begin{itemize}
    \item \textbf{Feedforward Neural Networks}: Very familiar and straightforward, used for approximating value functions and policies.
    \item \textbf{Convolutional Neural Networks (CNNs)}: Particularly effective for processing high-dimensional data with spatial structure, such as images. Widely used in tasks like playing video games from raw pixel inputs.
    \item \textbf{Recurrent Neural Networks (RNNs)}: Suitable for environments where the state has temporal dependencies. Using RNNs is one of the ways one can turn e Markovian again, i.e. independent of past states.
\end{itemize}

\section{Multi-Agent Reinforcement Learning}

When environments contain multiple agents that interact with each other, either cooperatively or competitively, \gls{MARL} is a discipline that extends the \gls{RL} principles to handle such cases.
Each agent in \gls{MARL} aims to optimize its own policy, considering the presence and actions of other agents, either explicitly or as part of the environment.
In domains where multiple decision-makers operate simultaneously and where one's decisions become the others' observations, \gls{MARL} is an indispensable tool for modeling these interactions.
These domains include robotics, autonomous driving, economics, and as we demonstrate in this work, cybersecurity.

In single-agent \gls{RL}, an agent learns a policy to maximize its cumulative reward through interacting with the environment.
This entails that the behavior of the agent alone is what determines the state transitions and rewards emitted by the environment.
In contrast, when environments encompass multiple agents each with its own policy and aligned or divergent objectives, learning becomes decidedly different.
The environment becomes dynamic as it now includes the actions of other learning agents, leading to several key differences from the single-agent case:

\begin{itemize}
    \item \textbf{Non-Stationarity}: Agents learn in an ever-changing environment, where other agents also constantly update their behavior. As the dynamics of the environment are influenced by the actions of all agents, it becomes less predictable and past experiences less accurate.
    \item \textbf{Partial Observability}: Agents have limited access to the states and rewards of other agents, if any at all. Depending on the nature of the task, this makes both cooperation and competition non trivial tasks.
    \item \textbf{Scalability}: The state and action spaces increase exponentially with the number of agents, complicating the learning task significantly.
\end{itemize}

The primary challenge however is the non-stationary nature of learning in these environments.
As an agent's interaction does influence the environment that other agents observe, it can lead to learning instability: the policies of any actively learning agents can oscillate due to the continually changing environment: traditional convergence guarantees of single-agent \gls{RL} algorithms do not hold.
To address these issues, several techniques have been developed:

\begin{itemize}
    \item \textbf{Joint Action Learning}: Agents learn a joint policy considering the actions of other agents, which can help in stabilizing learning.
    \item \textbf{Centralized Training with Decentralized Execution (CTDE)}: During training, a centralized entity has access to all agents information, enabling coordinated learning. During execution, each agent operates based on local observations and learned policies.
    \item \textbf{Opponent Modeling}: Agents explicitly model the policies of other agents and adapt their strategies accordingly.
    \item \textbf{Equilibrium Finding Algorithms}: Techniques such as fictitious play, where agents iteratively best respond to the average strategy of their opponents, can help convergence to equilibrium strategies.
\end{itemize}

\section{\gls{RL} in Cybersecurity}

What we usually mean by cybersecurity is the protection of computer systems, networks, and data from all types of attacks.
Towards that end, \gls{RL} provides indispensable tools for developing adaptive security strategies that can learn from interactions with attackers and dynamically respond to threats \cite{nguyen2021deep}.
Note however that the same applies for the threats themselves: they can employ the same tools to turn adaptive and defeat any deployed defenses, AI-enabled or not.
Even more so, the true efficacy of any adaptive defense is preconditioned on the adaptivity of any known adversaries.

While the game theoretic underpinnings of cybersecurity have long been established, the intrinsic relevance of \gls{RL} to cybersecurity and their strong correspondence does not come as a surprise.
Game theoretic concepts, especially those involving strategic interactions between defenders and attackers, play a crucial role in understanding and designing cybersecurity mechanisms.

\textbf{Nash Equilibrium.} A Nash Equilibrium is a situation in a game where no player can benefit by unilaterally changing their strategy, assuming the other players' strategies remain unchanged.
Formally, in a game with $n$ players, a strategy profile $\sigma^* = (\sigma_1^*, \sigma_2^*, \ldots, \sigma_n^*)$ is a Nash Equilibrium if for each player $i$:
\[
u_i(\sigma_i^*, \sigma_{-i}^*) \geq u_i(\sigma_i, \sigma_{-i}^*) \quad \forall \sigma_i \in \Sigma_i,
\]
where $u_i$ is the utility function of player $i$, $\Sigma_i$ is the strategy set of player $i$, and $\sigma_{-i}^*$ denotes the strategy profile of all players except $i$.

\textbf{Best-Response.} The best-response strategy for a player is the strategy that maximizes their utility given the strategies of the other players.
Formally, the best-response function $BR_i(\sigma_{-i})$ for player $i$ given the strategies $\sigma_{-i}$ of the other players is:
\[
BR_i(\sigma_{-i}) = \arg\max_{\sigma_i \in \Sigma_i} u_i(\sigma_i, \sigma_{-i}).
\]

\textbf{Zero-Sum Games}. A zero-sum game is a type of game in which the gain of a player is exactly balanced by the losses of other players.
In a two-player zero-sum game, the utilities $u_1$ and $u_2$ satisfy:
\[
u_1(\sigma_1, \sigma_2) + u_2(\sigma_1, \sigma_2) = 0 \quad \forall (\sigma_1, \sigma_2) \in \Sigma_1 \times \Sigma_2.
\]
These games are widely used in cybersecurity as the most common model of direct confrontation between attackers and defenders.

\textbf{General-Sum Games}. Zero-sum games are however not always representative of the possible interactions under competition.
To that end, general-sum games is a more general formalism that allows non-zero sum of payoffs where players can benefit -- or suffer -- simultaneously:
\[
u_1(\sigma_1, \sigma_2) + u_2(\sigma_1, \sigma_2) \neq 0.
\]
These games could be useful in modeling more complex interactions in cybersecurity that contain both competition and cooperation.

\textbf{Stackelberg Competition}. Stackelberg competition is a strategic game in which one player -- the leader -- makes a move before the other player -- the follower.
The leader commits to a strategy first, and then the follower optimizes their response based on the leader choice.
This is a widely adopted model for testing defenses in cybersecurity, where the defender assumes the position of the leader, and the attacker the follower.
A Stackelberg game can be formulated as follows:

\begin{itemize}
    \item The defender commits to a strategy $\sigma_D$.
    \item The attacker observes $\sigma_D$ and chooses a best response strategy $\sigma_A$ to maximize their payoff given $\sigma_D$.
    \item The defender's goal is to choose $\sigma_D$ that maximizes their own payoff, considering the attacker's best response.
\end{itemize}

Mathematically, this can be expressed as:
\[
\max_{\sigma_D \in \Sigma_D} \; U_D(\sigma_D, BR_A(\sigma_D))
\]
where $U_D$ is the defender's utility function and $BR_A(\sigma_D)$ is the best response of attacker given the strategy of the defender.
It is only by investigating these interactions as games that it is possible to develop optimal defenses, predict potential attacks, and ultimately enhance overall security posture.
Yet these game theoretic models have considerable limitations in direct application to cybersecurity use-cases, and are often unavoidably simplistic and unrepresentative of the interaction they attempt to model.

To formally solve a (cybersecurity) game, we require a complete model of the game together with an explicit description for all the interacting players and their utility functions in closed-form; something intractable for the vast majority of cybersecurity use-cases.
Unlike game theoretic formulations however, \gls{MARL} does not require such a model of the game and is therefore an indispensable tool for testing the security of real-world AI-enabled systems.

\subsection{Controllability and Observability}

When discussing adaptive adversarial attacks in cybersecurity, it is important to introduce the properties of controllability and observability from control theory.
Black-box attacks typically involve influencing a system without detailed knowledge of its internal workings, relying instead on observable behavior to devise optimal strategies.

Controllability in this context refers to the ability to manipulate the system inputs to achieve a desired effect, such as causing it to malfunction (evasion) or divulge sensitive information (membership inference).
Observability on the other hand refers to the ability to infer system states and vulnerabilities based on observed outputs, such as response times or error messages.

Thus a correspondence exists between effective black-box attacks and control theory.
An attacker must be able to both influence the system -- increase the controllability -- and accurately interpret the system responses to these influences -- increase the observability.
While \gls{RL} has the potential to learn optimal behaviors from interactions, to develop sophisticated attacks (and therefore effective defenses) we need to impart higher observability -- expressed through the state representation -- and higher controllability -- expressed through the action representation -- to our agents.
