\chapter{Background}\label{ch:background}
\label{ch:background}

This dissertation explores attacks and defenses against AI-based systems; it therefore lies at the intersection of cybersecurity and AI.
Cybersecurity, as is conventionally understood, refers to the practice of protecting systems, networks, and data from digital attacks, damage, or unauthorized access.
As our dependence on technology grows, so does the sophistication and frequency of cyber threats, and effective cybersecurity involves implementing strategies and defense mechanisms for combating a wide range of potential attacks.
These attacks are elicited through threat modeling, a systematic approach used to identify, assess, and prioritize potential threats to a system through the attack surface that it exposes.
A typical threat model includes information about the adversaries, more precisely their knowledge, goals, and capabilities, and the assets and defenses of the system under threat.

In this work we focus on well-defined subsets of adversarial goals and capabilities: the goal is to evade detection or cause a misclassification, while the capabilities are specifically against the logic implemented by the AI module.
By this we mean that adversaries can manipulate the information that is used as input to the system, and \textit{only} that; we consider attacks on the technical implementation of the system out of our scope.
In this chapter we introduce to the reader key concepts from \gls{AML} and \gls{RL}, scientific fields fundamental to our work, in order to facilitate a comprehensive understanding of this dissertation.

\section{Adversarial Machine Learning}
\gls{AML} is a field of study within machine learning \gls{ML} that focuses on the vulnerabilities of \gls{ML} models to adversarial attacks.
This field gained prominence in mid 2010s when researchers discovered that small, often imperceptible perturbations to input data could cause significant errors in ML model predictions \cite{biggio2013evasion, szegedy2013intriguing}.
These findings highlighted critical security issues in ML systems, that subsequently sparked a burgeoning scientific field and extensive research into understanding, exploiting, and ultimately defending against these vulnerabilities.

After the initial inception, adversarial attacks on ML models enjoyed a prolific growth, where researchers conceived and performed attacks on every context or model available, with often little regard to how feasible or realistic these attacks might be.
To provide an overview of adversarial attacks on \gls{ML}, we can categorize them into several types: according to the level of access they have to the model, as well as the objective of the attack.

\subsection{Adversarial Objective}

While not intended as an exhaustive list, adversarial attacks fall largely in four separate categories:

\begin{itemize}
    \item \textbf{Evasion Attacks}. These are attacks that occur at test time. An adversary slightly modifies input data to ``evade'' detection or classification. For instance, adding noise to an image can mislead an image classifier into making an incorrect prediction.
    \item \textbf{Poisoning Attacks}. Contrary to evasion, poisoning attacks occur at training time. The attacker injects malicious data into the training set, aiming to corrupt the learning process. This aim can be the degradation of the model's performance or the insertion of backdoors (triggers) that enable a special kind of evasion at test time.
    \item \textbf{Model Extraction Attacks}. In model extraction attacks, the adversary aims to steal the parameters or hyperparameters of a model. By querying the model and analyzing the outputs, an attacker can approximate the original model.
    \item \textbf{Privacy Attacks}. Attacks like membership/attribute inference and data reconstruction can lead to privacy breaches, revealing sensitive information about individuals or their attributes in the dataset.
\end{itemize}

\textbf{Adversarial Examples.} The most common attack in \gls{AML} works is evasion, and the samples that are part of such an attack or the results of it, are known as adversarial examples.
Adversarial examples are inputs to \gls{ML} models that are intentionally designed to cause the model to make a mistake.
These inputs are typically created by adding small, carefully crafted perturbations to legitimate, clean examples; these modifications are almost imperceptible to humans but cause significant errors in the model predictions.
The study of adversarial examples has revealed vulnerabilities in various \gls{ML} models, particularly in deep neural networks, raising concerns about their robustness and security in real-world applications.

Formally, let \( f: \mathbb{R}^n \to \mathbb{R}^m \) be a machine learning model, where \( f(x) \) represents the model's output for an input \( x \in \mathbb{R}^n \). Consider a legitimate input \( x \) with true label \( y \). An adversarial example \( x' \) is a perturbed version of \( x \) such that:

\begin{enumerate}
    \item \( x' = x + \delta \), where \( \delta \in \mathbb{R}^n \) is the perturbation. 
    \item The perturbation \( \delta \) is constrained to be smaller than a threshold \(\epsilon\), often measured by a norm \( \|\delta\|_p \leq \epsilon\), for some norm \( p \); common norm choices are \( p = 2 \) (Euclidean) or \( p = \infty \) (maximum).
    \item The perturbed input \( x' \) leads to a misclassification by the model, i.e., \( f(x') \neq y \) or, more generally, \( f(x') \neq f(x) \).
\end{enumerate}

In practice, adversarial examples are often generated by optimizing the perturbation \( \delta \) to maximize the prediction error of the model, subject to the perturbation constraint.
Formally, an adversarial attack has to solve the following optimization problem:
\[
\max_{\delta} \mathcal{L}(f(x + \delta), y)
\]
subject to
\[
\|\delta\|_p \leq \epsilon,
\]
where $\mathcal{L}$ is the loss function, $f$ is the model, $x$ is the original input, $y$ is the true label, $\|\cdot\|_p$ is the $p$-norm, and $\epsilon$ is the small constant that bounds the perturbation.

While adversarial examples were initially discovered and subsequently researched in the image classification domain, eventually they appeared in every domain that employs supervised, unsupervised, and reinforcement learning.
Naturally, different domains will have distinct constraints; in adversarial malware for example, the similarity to the original sample is of less importance, while the preservation of semantics and functionality is of high importance. 

\subsubsection{White-Box vs. Black-Box Optimization}

In white-box adversarial attacks, the attacker has complete access to the model, including its architecture, parameters, and computational graph.
This allows for an end-to-end, gradient-based optimization of the perturbations that can mislead the model.
Common methods for white-box attacks include Fast Gradient Sign Method (\gls{FGSM}) and Projected Gradient Descent (\gls{PGD}).

In contrast, in black-box adversarial attacks the adversary does not have access to the internals of the model, a more apt and representative model of cybersecurity environments.
Instead, the one capability that is typically preserved for the adversary in black-box contexts, is the ability to query the model and observe the output.

The lack of access to the underlying model and lack of a closed-form description of it, makes it challenging to compute gradients, at least directly.
Alternative strategies need to be employed for optimizing the attack: such approaches include the approximation of the gradient with techniques like finite differences or surrogate models, or derivative free optimization like genetic algorithms.

\subsection{Mitigations and Defenses}

Over time, several defense mechanisms and mitigations to adversarial examples have been proposed and tested:

\begin{itemize}
    \item \textbf{Adversarial Training}. This defense incorporates adversarial examples into the training process to enhance model robustness. A computationally intensive approach but with significant efficacy.
    \item \textbf{Defensive Distillation}. This technique involves training a model to output probabilities of class labels, which can help in smoothing the decision boundary and making the model less sensitive to adversarial perturbations.
    \item \textbf{Gradient Masking}. Hiding or obfuscating the gradient information makes it harder for attackers to craft adversarial examples. However, this approach has been shown to be often ineffective against stronger attacks.
    \item \textbf{Ensemble Methods}. Using multiple models and aggregating their predictions can provide a level of robustness, as it is less likely that all models will be equally vulnerable to the same adversarial examples.
    \item \textbf{Feature Squeezing / Denoising}. Reducing the complexity of the input space (e.g., by bit-depth reduction) or by using denoising methods, a defense has the potential to remove adversarial perturbations.
\end{itemize}

We should note here that all these defenses confer only empirical robustness to a model; so far there have been no definitive proofs of a model's operational robustness at deployment.
Furthermore, out of all these defenses the only one that has withstood the test of time and is still widely used is adversarial training~\cite{tramer2020adaptive}.
Adversarial training can be mathematically formulated as a min-max optimization problem:

\begin{equation}
\min_{\theta} \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \max_{\delta \in \Delta} L(f_\theta(x + \delta), y) \right],
\end{equation}

where \( \theta \) represents the model parameters, \( \mathcal{D} \) is the data distribution, \( \Delta \) is the set of permissible perturbations, \( L \) is the loss function, and \( f_\theta \) is the model.
The inner maximization seeks the worst-case perturbation \( \delta \), while the outer minimization adjusts the model parameters \( \theta \) to minimize the loss; it is essentially a saddle-point optimization task.
As both the loss landscape and the set of perturbations are non-convex, adversarial training is typically challenging and computationally intensive, that can lead to instability and convergence issues due to the alternating nature of the optimization.

\subsection{Shortcut Learning}

Shortcut learning refers to models leveraging non-generalizable patterns and spurious correlations in the training data to make predictions~\cite{geirhos2020shortcut}.
These shortcuts can make models vulnerable to adversarial attacks and lead to poor performance on out-of-distribution samples.
\gls{OOD} samples should be considered a given in environments like cybersecurity, due to the involved agency of adversaries trying to evade the model.
Adversaries introduce small perturbations to exploit these shortcuts that the model erroneously relies on for prediction.

When \gls{ML} models are trained without adversarial training or without considering robustness to attacks, then they are more likely to depend on any shortcuts or artifacts present in the data, and even be more vulnerable to attacks~\cite{ilyas2019adversarial}.
As a result, these models will perform poorly when exposed to adversarial examples and other forms of distributional shifts, limiting their real-world applicability and reliability.
It is thus paramount to identify in advance any reliance on non-robust features (e.g. by adversarial training) and mitigate them.

\section{Reinforcement Learning}

\gls{RL} is a discipline of \gls{AI} that formalizes agents learning how to make optimal decisions by interacting with an environment.
The agent takes actions in an environment, receives feedback in the form of rewards, and adjusts its actions to maximize the cumulative rewards over time.
This reinforced trial-and-error approach allows the agent to discover optimal behaviors through experience rather than by being explicitly programmed.

The foundational concept in \gls{RL} is the \gls{MDP}, which provides a mathematical formalism for modeling the decision-making process in an environment.
An \gls{MDP} consists of states, actions, rewards, and state transition probabilities, and it assumes the Markov property where the future state depends only on the current state and action.

\gls{RL} differs significantly from disciplines like supervised learning and unsupervised learning.
In supervised learning, an algorithm learns from a labeled dataset, and in unsupervised learning, it discovers patterns in data without explicit labels.
\gls{RL}, however, deals with learning from the consequences of actions, making it suitable for problems where an agent must make a sequence of decisions.

Applications of \gls{RL} span across diverse fields including robotics, where robots learn to navigate and manipulate objects; games, where agents learn to play and excel at from chess to Go and video games; finance, where trading strategies are optimized; and as it most prominently featured in this dissertation, cybersecurity, where agents learn how to attack or defend AI-based systems.
The versatility and generality of \gls{RL} have made it a prominent area of research, even a model for how humans learn, as we collectively accelerate towards \gls{AGI}.

Despite its promise, \gls{RL} poses several challenges that oft make it a bad choice to apply it uncritically and out-of-the-box.
Such challenges are the exploration-exploitation trade-off, where an agent must balance exploring the environment to discover new strategies and exploiting known strategies to maximize rewards; sparse or delayed rewards, where an agent following a suboptimal initial policy might never reach them; and others like the design of reward functions, the scalability to high-dimensional state and action spaces, and ensuring stability and convergence of learning algorithms, which all are crucial ongoing research subjects.

In this chapter, we delve into the theoretical underpinnings of \gls{RL}, introduce the prominent \gls{RL} algorithms, and examine their applications and challenges.
We will also discuss topics such as function approximation with neural networks, multi-agent reinforcement learning (\gls{MARL}), and important concepts that lie at tbe intersection of \gls{RL} and cybersecurity.

\section{Markov Decision Processes}
\label{sec:mdp}

Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making problems where outcomes are partly random and partly under the control of a decision-maker.
MDPs are widely used in reinforcement learning to formalize the environment in which an agent operates.

An MDP is defined by the tuple $(S, A, P, R, \gamma)$, where:
\begin{itemize}
    \item $S$ is a finite set of states.
    \item $A$ is a finite set of actions.
    \item $P: S \times A \times S \rightarrow [0,1]$ is the state transition probability function, where $P(s'|s,a)$ denotes the probability of transitioning to state $s'$ from state $s$ after taking action $a$.
    \item $R: S \times A \rightarrow \mathbb{R}$ is the reward function, where $R(s,a)$ denotes the immediate reward received after taking action $a$ in state $s$.
    \item $\gamma \in [0,1]$ is the discount factor, which determines the importance of future rewards.
\end{itemize}

The objective in an MDP is to ``solve'' it, that is to find a policy $\pi: S \rightarrow A$ that maximizes the expected cumulative reward, also known as the return.
The return $G_t$ at time $t$ is defined as:
\[
G_t = \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k})
\]
where $s_{t+k}$ and $a_{t+k}$ are the states and actions at time $t+k$, respectively.

\subsection{Value Functions}

Value functions are used to estimate the expected return from a given state or state-action pair under a specific policy.
The state-value function $V^{\pi}(s)$ for policy $\pi$ is defined as the expected return when starting in state $s$ and following policy $\pi$ thereafter:
\[
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s \right]
\]

The action-value function $Q^{\pi}(s, a)$ for policy $\pi$ is defined as the expected return when starting in state $s$, taking action $a$, and then following policy $\pi$ thereafter:
\[
Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s, a_t = a \right]
\]

Typically, the goal in \gls{RL} problems is to find the optimal policy $\pi^*$ that maximizes the value function:
\[
\pi^* = \arg\max_{\pi} V^{\pi}(s) \quad \forall s \in S
\]

\textbf{Advantage Functions.} In place of the value function, the advantage function $\hat{A}_t$ is sometimes used; this is crucial for reducing the variance in policy gradient methods for example.
The advantage (in choosing between actions) can be estimated in various ways, such as in Generalized Advantage Estimation (GAE) that provides a balance between bias and variance:
\[
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\]
where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the temporal difference error, and $\lambda$ is a hyperparameter that controls the trade-off.

\subsection{Bellman Equations}

The Bellman equations provide recursive definitions for the value functions.
For the state-value function, the Bellman equation is defined as:
\[
V^{\pi}(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \left[ R(s,a) + \gamma V^{\pi}(s') \right]
\]

For the action-value function, the Bellman equation is:
\[
Q^{\pi}(s, a) = R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) \sum_{a' \in A} \pi(a'|s') Q^{\pi}(s', a')
\]

The optimal state-value function $V^*(s)$ and the optimal action-value function $Q^*(s, a)$ satisfy the Bellman optimality equations:
\[
V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \left[ R(s,a) + \gamma V^*(s') \right]
\]

\[
Q^*(s, a) = R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) \max_{a' \in A} Q^*(s', a')
\]

Having defined the action-value and state-value functions, the two fundamental approaches for finding their optimal policy and thus solving the MDPs, are called policy iteration and value iteration.

\textbf{Policy Iteration}. This approach is composed of two steps, policy evaluation and policy improvement:
\begin{itemize}
    \item \textbf{Policy Evaluation}: Calculate the value function $V^{\pi}$ for the current policy $\pi$ by solving the Bellman equation.
    \item \textbf{Policy Improvement}: Update the policy $\pi$ by choosing actions that maximize the value function.
\end{itemize}

By alternating between these two, the algorithm converges to the optimal policy $\pi^*$.

\textbf{Value Iteration}. This approach simplifies policy iteration by combining policy evaluation and policy improvement into a single step.
It iteratively updates the value function using the Bellman optimality equation:
\[
V_{k+1}(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s,a) \left[ R(s,a) + \gamma V_k(s') \right]
\]

The algorithm converges to the optimal value function $V^*$, and the optimal policy $\pi^*$ can be derived from $V^*$.


\section{RL Algorithms}

As \gls{MDP}s can be solved in various ways, it is important to understand how different \gls{RL} algorithms achieve that.
These algorithms can be broadly categorized based on two criteria.
One such distinction is between model-free and model-based methods, while another is between on-policy and off-policy methods.
Understanding these categories is important as it helps in selecting the appropriate algorithm for a given problem and environment.

\subsubsection{Model-Free vs Model-Based}

Model-free are algorithms that do not attempt to learn a model of the environment or its dynamics; instead, they focus directly on learning the policy or the value function.
While model-free algorithms are generally simpler and require fewer assumptions, they might need more interactions with the environment to learn effective policies.
The vast majority of applied RL uses model-free methods, further subdivided to value-based methods like Q-learning and SARSA, and policy-based methods which directly learn the policy, such as REINFORCE and actor-critic.

On the other hand, model-based algorithms attempt to learn a model of the dynamics of the environment, which includes the state transition probabilities and the reward function.
These models are then used for planning and decision-making.
Model-based algorithms can be more powerful and data-efficient but also more complicated, as they require a model of the environment; model-free algorithms on the other hand are more straightforward to implement.

\subsubsection{On-Policy vs Off-Policy Algorithms}

On-policy algorithms learn the value of the policy that is currently being used to make decisions. 
The policy is evaluated and improved continually, based on the actions taken by the agent.

Off-policy algorithms, in contrast, learn the value of the optimal policy independently of the agent actions.
This allows off-policy methods to reuse experience generated from older policies, at the cost of less accurate updates.
Off-policy algorithms can additionally employ importance sampling, a technique used to correct the bias introduced by using experiences from a different policy.

Here, we provide a (non-exhaustive) list of the most commonly used algorithms in RL:

\begin{itemize}
    \item \textbf{Q-learning}: A popular model-free and off-policy algorithm that learns the action-value function $Q(s, a)$.
    \[
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
    \]
    \item \textbf{Deep Q-Networks (DQN)}: An extension of Q-learning that uses deep neural networks to approximate the action-value function. It leverages experience replay and target networks to stabilize training.
    \[
    Q(s, a; \theta) \approx Q^*(s, a)
    \]
    \item \textbf{SARSA (State-Action-Reward-State-Action)}: An on-policy algorithm that updates the action-value function based on the action taken by the policy.
    \[
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
    \]
    \item \textbf{REINFORCE}: A Monte Carlo policy gradient algorithm that updates the policy parameters $\theta$.
    \[
    \theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) G_t
    \]
    \item \textbf{Actor-Critic}: Combines value-based and policy-based methods by having two components: an actor that updates the policy and a critic that evaluates the policy.
    \[
    \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
    \]
    \[
    \theta \leftarrow \theta + \alpha \delta_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
    \]
    \[
    w \leftarrow w + \beta \delta_t \nabla_w V(s_t)
    \]
    \item \textbf{Dyna-Q}: A model-based hybrid algorithm that integrates learning, planning, and acting. It learns the model of the environment and uses it to simulate experiences.
    \[
    P(s'|s,a) \leftarrow \text{update model from real experience}
    \]
    \[
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
    \]
    \item \textbf{AlphaZero}: A state-of-the-art model-based algorithm that uses Monte Carlo Tree Search (MCTS) for planning and neural networks to represent the policy and value functions. It learns from self-play by iteratively improving the model.
\end{itemize}

\section{Proximal Policy Optimization}

\gls{PPO} is a state-of-the-art \gls{RL} algorithm that combines the stability of trust region methods with the scalability of policy gradient methods.
Trust regions methods, like Trust Region Policy Optimization (TRPO), ensure that the policy updates are not too large by imposing a Kullback-Leibler (KL) divergence constraint.
Policy gradient methods like REINFORCE, make policy updates that are proportional to the gradient of the log probability of the actions taken, weighted by the rewards obtained.
To achieve stable and reliable policy updates, \gls{PPO} employs either a clipped surrogate objective (PPO-clip) or a KL-penalty objective (PPO-penalty).

\textbf{PPO-clip}. It uses a clipped surrogate objective to ensure stability, and is defined as follows:
\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
\]
where:
\begin{itemize}
    \item $r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ is the probability ratio between the new policy $\pi_{\theta}$ and the old policy $\pi_{\theta_{\text{old}}}$.
    \item $\hat{A}_t$ is the advantage function, which estimates the relative value of action $a_t$ compared to the average action at state $s_t$.
    \item $\epsilon$ is a hyperparameter that controls the size of the trust region.
\end{itemize}

The clipped objective ensures that the policy updates are constrained, thereby maintaining stability and preventing drastic changes that could degrade performance.

\textbf{PPO-penalty} Unlike PPO-clip, PPO-penalty promotes smoother updates by penalizing large deviations from the old policy by introducing a KL divergence penalty term to the objective function, which is written as:
\[
L^{\text{KL}}(\theta) = \mathbb{E}_t \left[ r_t(\theta) \hat{A}_t - \beta \text{KL}[\pi_{\theta_{\text{old}}}(\cdot|s_t) \| \pi_{\theta}(\cdot|s_t)] \right]
\]
where:
\begin{itemize}
    \item $\text{KL}[\pi_{\theta_{\text{old}}}(\cdot|s_t) \| \pi_{\theta}(\cdot|s_t)]$ is the KL divergence between the old policy and the new policy.
    \item $\beta$ is a coefficient that controls the importance of the KL divergence term.
\end{itemize}

Both PPO-clip and PPO-penalty alternate between sampling data through interaction with the environment and optimizing the surrogate objective using Stochastic Gradient Descent (\gls{SGD}).
The steps are shown in Algorithm \ref{alg:PPOgen}.

\begin{algorithm}
\caption{Proximal Policy Optimization (PPO)}
\label{alg:PPOgen}
\begin{algorithmic}[1]
\STATE Initialize policy parameters $\theta_0$ and value function parameters $\phi_0$.
\FOR{each iteration}
    \STATE Collect a set of trajectories $\{(s_t, a_t, r_t, s_{t+1})\}$ by running the current policy $\pi_{\theta_k}$ in the environment.
    \STATE Compute advantage estimates $\hat{A}_t$ based on the collected trajectories.
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \STATE Optimize the PPO objective $L^{\text{CLIP}}(\theta)$ or $L^{\text{KL}}(\theta)$ with respect to $\theta$ using the collected data, to update the policy parameters:
    \[
    \theta_{k+1} = \theta_k + \alpha \nabla_\theta L(\theta)
    \]
    \STATE Update the value function by minimizing the mean-squared error loss:
    \[
    L^{\text{VF}}(\phi) = \mathbb{E}_t \left[ (V_{\phi}(s_t) - R_t)^2 \right]
    \]
\ENDFOR
\end{algorithmic}
\end{algorithm}

In summary, \gls{PPO} is a method that combines the advantages of policy gradients with stability enhancements through its clipped or penalized objective function.
The use of neural networks for function approximation enables it to handle continuous and high-dimensional state and action spaces, making it a powerful \gls{RL} tool for modern cybersecurity environments.
Due to its advantages, \gls{PPO} is the most widely used RL algorithm, and is featured in all the individual works of this dissertation.

\subsection{Function Approximation with Neural Networks}

In complex environments with large or continuous state and action spaces, traditional tabular methods for representing value functions or policies become intractable.
Neural networks provide a powerful tool for approximating these functions, enabling RL algorithms to scale to high-dimensional problems.
Different architectures of neural networks can be used depending on the characteristics of the environment and the task.
Such architectures include:

\begin{itemize}
    \item \textbf{Feedforward Neural Networks}: Very familiar and straightforward, used for approximating value functions and policies.
    \item \textbf{Convolutional Neural Networks (CNNs)}: Particularly effective for processing high-dimensional data with spatial structure, such as images. Widely used in tasks like playing video games from raw pixel inputs.
    \item \textbf{Recurrent Neural Networks (RNNs)}: Suitable for environments where the state has temporal dependencies. Using RNNs allows for capturing information from previous states, making them a good choice for tasks where the state is not Markovian.
\end{itemize}

\section{Multi-Agent Reinforcement Learning}

When environments contain multiple agents that interact with each other, either cooperatively or competitively, \gls{MARL} is a discipline that extends the \gls{RL} fundamentals to such cases.
Each agent in \gls{MARL} aims to optimize its own policy, considering the presence and actions of other agents, either explicitly or as part of the environment.
In domains where multiple decision-makers operate simultaneously and where one's decisions become the others' observations, \gls{MARL} is an indispensable tool for modeling these interactions.
Examples of such domains are robotics, autonomous driving, economics, and the primary focus of this work, cybersecurity.

In single-agent \gls{RL}, an agent learns a policy to maximize its cumulative reward through interacting with the environment.
This entails that the behavior of the agent alone is what determines the state transitions and rewards emitted by the environment.
In contrast, when environments encompass multiple agents, each with its own policy and objectives that are aligned or divergent, the learning process becomes decidedly different.
The environment becomes dynamic as it now includes the actions of other learning agents, leading to several key differences from the single-agent case:

\begin{itemize}
    \item \textbf{Non-Stationarity}: Agents learn in an ever-changing environment, where other agents also constantly update their behavior. As the dynamics of the environment are influenced by the actions of all agents, it becomes less predictable and past experiences less accurate.
    \item \textbf{Partial Observability}: Agents have limited access to the states and rewards of other agents, if any at all. Depending on the nature of the task, this makes both cooperation and competition non-trivial tasks.
    \item \textbf{Scalability}: The state and action spaces increase exponentially with the number of agents, complicating the learning task significantly.
\end{itemize}

The primary challenge however is the non-stationary nature of learning in these environments.
As an agent's interactions do influence the environment that other agents observe, it can lead to instability in training: the policies of any actively learning agents can oscillate due to the continually changing environment as traditional convergence guarantees of single-agent \gls{RL} algorithms do not hold.
To address these issues, several techniques have been developed:

\begin{itemize}
    \item \textbf{Joint Action Learning}: Agents learn a joint policy considering the actions of other agents, which can help in stabilizing learning.
    \item \textbf{Centralized Training with Decentralized Execution (CTDE)}: During training, a centralized entity has access to all agent information, enabling coordinated learning. During execution, each agent operates based on local observations and learned policies.
    \item \textbf{Opponent Modeling}: Agents explicitly model the policies of other agents and adapt their strategies accordingly.
    \item \textbf{Equilibrium Finding Algorithms}: Techniques such as fictitious play, where agents iteratively best respond to the average strategy of their opponents, can help to converge to equilibrium strategies.
\end{itemize}

\section{RL in Cybersecurity}

What we usually signify by cybersecurity is the protection of computer systems, networks, and data, from all kinds of attacks.
Towards that end, \gls{RL} provides indispensable tools for developing adaptive security strategies that can learn from interactions with attackers and dynamically respond to threats \cite{nguyen2021deep}.
Note however that the same applies for the threats themselves: they can employ the same tools to turn adaptive and overwhelm any deployed defenses, AI-enabled or not.
Expressed differently, the real efficacy of any adaptive defense is preconditioned on the adaptivity of the known adversaries.

It has been long established that cybersecurity has several game-theoretic underpinnings; therefore the intrinsic relevance of and strong correspondence with \gls{RL} does not come as a surprise.
Game theoretic concepts, especially those involving strategic interactions between defenders and attackers, play a crucial role in understanding and designing cybersecurity mechanisms.

\textbf{Nash Equilibrium.} A Nash Equilibrium is a situation in a game where no player can benefit by unilaterally changing their strategy, assuming the other players' strategies remain unchanged.
Formally, in a game with $n$ players, a strategy profile $\sigma^* = (\sigma_1^*, \sigma_2^*, \ldots, \sigma_n^*)$ is a Nash Equilibrium if for each player $i$:
\[
u_i(\sigma_i^*, \sigma_{-i}^*) \geq u_i(\sigma_i, \sigma_{-i}^*) \quad \forall \sigma_i \in \Sigma_i,
\]
where $u_i$ is the utility function of player $i$, $\Sigma_i$ is the strategy set of player $i$, and $\sigma_{-i}^*$ denotes the strategy profile of all players except $i$.

\textbf{Best-Response.} The best-response strategy for a player is the strategy that maximizes their utility given the strategies of the other players.
Formally, the best-response function $BR_i(\sigma_{-i})$ for player $i$ given the strategies $\sigma_{-i}$ of the other players is:
\[
BR_i(\sigma_{-i}) = \arg\max_{\sigma_i \in \Sigma_i} u_i(\sigma_i, \sigma_{-i}).
\]

\textbf{Zero-Sum Games}. A zero-sum game is a type of game in which the gain of a player is exactly balanced by the losses of other players.
In a two-player zero-sum game, the utilities $u_1$ and $u_2$ satisfy:
\[
u_1(\sigma_1, \sigma_2) + u_2(\sigma_1, \sigma_2) = 0 \quad \forall (\sigma_1, \sigma_2) \in \Sigma_1 \times \Sigma_2.
\]
These games are widely used in cybersecurity as the most common model of direct confrontation between attackers and defenders.

\textbf{General-Sum Games}. Zero-sum games are however not always representative of the possible interactions under competition.
To that end, general-sum games is a more general formalism that allows non-zero sum of payoffs where players can benefit -- or suffer -- simultaneously:
\[
u_1(\sigma_1, \sigma_2) + u_2(\sigma_1, \sigma_2) \neq 0.
\]
This kind of games is useful when modeling more complex scenarios and interactions containing both competition and cooperation, which arise both in real-life and cybersecurity.

\textbf{Stackelberg Competition}. Stackelberg competition is a strategic game in which one player -- the leader -- makes a move before the other player -- the follower.
The leader commits to a strategy first, and then the follower optimizes and strategizes their response, based on what the leader committed to.
This is a widely adopted model for testing defenses in cybersecurity, where the defender assumes the position of the leader, and the attacker the follower.
A Stackelberg game can be formulated as follows:

\begin{itemize}
    \item The defender commits to a strategy $\sigma_D$.
    \item The attacker observes $\sigma_D$ and chooses a best response strategy $\sigma_A$ to maximize their payoff given $\sigma_D$.
    \item The defender's goal is to choose $\sigma_D$ that maximizes their own payoff, consideringx the attacker's best response.
\end{itemize}

Mathematically, this is expressed as:
\[
\max_{\sigma_D \in \Sigma_D} \; U_D(\sigma_D, BR_A(\sigma_D))
\]
where $U_D$ is the defender's utility function and $BR_A(\sigma_D)$ is the best response of attacker given the strategy of the defender.

It is by investigating these interactions as games that it becomes possible to develop optimal defenses, resist potential attacks, and ultimately enhance the overall security posture of a system.
Yet there are considerable limitations in the formalization of such game theoretic models: when applied directly to cybersecurity use-cases, they typically contain simplifications unrepresentative of the interaction they are supposed to model.

To formally solve a (cybersecurity) game, one requires a complete model of the game together with an explicit description for all the interacting players and their utility functions in closed-form; something intractable for the vast majority of cybersecurity use-cases.
Unlike game theoretic formalization however, \gls{RL} and \gls{MARL} do not require the model of the game and are therefore indispensable tools for testing the security of real-world AI-enabled systems.

\subsection{Controllability and Observability}
\label{sec:control}

When discussing adaptive adversarial attacks in cybersecurity, it is important to introduce the properties of controllability and observability from control theory.
Black-box attacks typically involve influencing a system without detailed knowledge of its internal workings, relying instead on observable behavior to devise optimal strategies.

Controllability in this context refers to the ability to manipulate the system inputs to achieve a desired effect, such as causing it to malfunction (e.g. evasion) or divulge sensitive information (e.g. membership inference).
Observability on the other hand refers to the ability to infer system states and vulnerabilities based on observed outputs, such as response times or error messages.

Therefore, there is a correspondence between effective black-box attacks and adaptive control.
An attacker must be able to both influence the system -- increase the controllability -- and accurately interpret the system responses to these influences -- increase the observability.
\gls{RL} has the potential to learn optimal policies from interactions, however to develop sophisticated attacks and subsequently effective defenses, we need to impart to our agents better observability and controllability; the former is achieved via the state representation, while the latter by expanding the available action set.

\section{Use of Generative AI}

No generative AI assistance tools were used during the research or writing process of this dissertation, except for proofreading.
The text, code, and images in this thesis are my own and generative AI has only been used in accordance with the KU Leuven guidelines and appropriate references have been added.
I have reviewed and edited the content as needed and I take full responsibility for the content of the thesis.
