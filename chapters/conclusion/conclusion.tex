% !TeX root = ../../thesis.tex
\chapter{Conclusion}\label{ch:conclusion}

\epigraph{"Resistance and change often begin in art.\\Very often in our art, the art of words."}{--- Ursula Le Guin}
% The artist deals in what cannot be said in words. The artist whose medium is fiction does this in words. 

The widespread and accelerating adoption of \gls{AI} across various domains is gradually transforming them, and in turn precipitates a fundamental shift on how we approach and interact with them. 
From cybersecurity, to finance, healthcare, media, and education, AI is becoming instrumental to the operation of many critical applications.
This in turn broadens the attack surface as novel threats and failure modes are introduced, like model extraction, model poisoning, and most prominently, model evasion.
Furthermore, there are risks and threats that lie beyond the model level and should not be underestimated.
While the fundamentals of modern AI have been around for a long time and endured two \gls{AI} winters, the latest progress has been fueled by advances in computation and the vast amounts of available data.
Notably, this abundance of data stems from our aggregate activity and digital footprint, with much of it drawn from publicly accessible sources such as Wikipedia and Stack Overflow\footnote{https://arstechnica.com/information-technology/2024/05/stack-overflow-users-sabotage-their-posts-after-openai-deal/}.
This wealth of information is then distilled into models that encapsulate our collective knowledge and reasoning abilities, with the process and the resulting models frequently being proprietary and closed-source.

While these advances are impressive from various perspectives, such as the potential leaps in productivity, the (so far false) promise of automation, and the incremental steps towards the elusive goal of \gls{AGI}, they also raise significant concerns on the security, safety, and accountability of \gls{AI} solutions, as well as on ethical and societal implications.
It is commonplace for \gls{AI} models to be trained and deployed with minimal consideration for their resilience against adversarial attacks.
Inherently adversarial contexts, like cybersecurity, exacerbate the generalization challenges these models already face by inducing worst-case scenarios and distribution shifts, which can severely impair their performance.
In this dissertation we contribute a methodological approach for investigating and evaluating the robustness of AI models that expose an interface to the real world, across a wide range of domains and modalities: from web automation and adversarial malware to the primary \gls{AML} testbed, image classification.

Across the three individual works presented, we arrive to several important insights and takeaways.
First, as \gls{AI} capabilities continue to advance, the competition between automation and its detection ultimately tilts in favor of the former, even when detection mechanisms remain black-box with their internal workings concealed.
This suggests that in the future the security of bot detection will \textit{have} to rely on obscurity.
Secondly, performance on clean, non-adversarial test sets often fails to accurately reflect a model's true generalization capabilities.
This underlines the importance of adversarial training, as a process of identification and removal of spurious correlations and brittle features that adversaries can exploit for evasion.
Furthermore, in certain domains like image classification, model hardening defenses like adversarial training can prove insufficient, as adaptive adversaries can circumvent them.
In these cases, active and adaptive defenses are necessitated as a complement.
Finally, resilience in cybersecurity is achieved by fully adaptive and optimized attacks, a prerequisite for uncovering their full potential.
These are essential for uncovering counterfactuals to what the model has learned, enabling model hardening, as well as to develop effective countermeasures through adaptive defenses.

The ever-growing integration of \gls{AI} in the decision-making process of various systems introduces substantial security risks, particularly when these systems can be interfaced with.
A recurring theme of this dissertation is that simply having such an exposed interface, one that can be queried and interacted with, suffices for attackers to exploit the underlying model in various ways.
This exploitation extends further than manipulating the model decisions: from compromising the privacy of other users, to disrupting its availability, services, and undermining the model's overall functionality.
However, as we increasingly shift parts of our decision-making to AI, new vulnerabilities are introduced where lapses in security and correct specification can have far-reaching consequences, impacting individuals, institutions, and society at large. 
The environments in which these AI models operate form closed feedback loops with their users, meaning that while we train and shape \gls{AI} models through the data we generate, we are, in turn, trained by these systems.
Looking ahead, we must confront not only the prospect of adversarial attacks against \gls{AI}, but also the adversarial use of \gls{AI} against individuals, from the seemingly innocuous like engagement optimization to the brazenly antisocial like large-scale misinformation campaigns.
As we make strides forward as both a research community and society, we must remain vigilant to the novel risks introduced.
\section{Summary of Contributions}
This dissertation is composed by a comprehensive examination of the inherent vulnerabilities and correct functioning of black-box, AI-based systems, in diverse applications and domains.
Specifically, we investigated: a) the most widely adopted Captcha service that tells humans and bots apart, b) dynamic analysis based malware detection and how to harden it against evasion, and c) adaptive attacks, defenses, and the general competitive game they form in image classification.
In these three distinct yet interconnected cases, we reveal the complexities and idiosyncrasies of the interactions between offensive and defensive methods, in order to guarantee that the underlying models function according to their specification.

Our investigation into the Google reCAPTCHA v3 service highlighted its susceptibility to adversarial attacks, where the web browsing automation framework we developed reaches an evasion rate up to 99.6\%.
While bot detection that differentiates humans from bots through continuous behaviometric evaluation is and will remain an indispensable tool, our study highlights that it also inadvertently creates the conditions of its own obsolescence.
The risk scores generated are influenced by static aspects, for instance privacy settings and IP addresses, but also by the dynamic behavior of the user.
We demonstrate how the latter can be exploited, and develop \gls{RL} agents can simulate human-like browsing behavior that successfully evades detection.
Our findings underline the importance of developing more resilient Captcha for enhanced bot and automation detection, potentially ones that move beyond current zero-friction, zero-challenge systems.

Our second study delves into the resilience of dynamic analysis based malware detection, specifically when ML-based models are employed against adversarial malware.
We introduce AutoRobust, a general methodology that leverages \gls{RL} to harden a model against evasion, and to identify and mitigate spurious correlations and brittle features.
We automate this demanding, human-in-the-loop process, by substituting the requirement for expert input on every potential shortcut feature, with asking a simple question: what are all the allowed ways something could be different and \textit{still} be the same thing?
Our approach imparts a probabilistic level of robustness against well-defined adversarial capabilities, thereby ensuring robust model performance \textit{without} compromising the efficacy on clean inputs.
Notably, our approach assumes nothing about the underlying model \textit{or} the specific problem-space capabilities that are used to generate adversarial malware, it is thus general and applicable to any attack when gradients cannot be computed with respect to the inputs.
Our study illustrates that adversarial attacks on malware detection should be performed through the problem space, highlighting the importance of continuous adaptation and hardening of the detection models.

Our third study broadens our scope by examining decision-based attacks and defenses, as well as how both can adapt to each other.
First, we discover that adaptive attackers can perform exceedingly well, even against adversarially trained models; their effectiveness rests then on their ability to adapt the policies that govern their operation and their evasive arsenal \textit{in tandem}.
We introduce a framework named Adversarial Markov Games (AMG), which integrates, studies, and evaluates both theoretically and empirically decision-based attacks and defenses, first independently but also at their intersection.
We demonstrate that active defenses, which control how the system responds, are a necessary complement to model hardening when facing decision-based attacks; then how these defenses can be circumvented by adaptive attacks, only to finally elicit active and adaptive defenses.
Through our AMG framework we simplify the intricate task of learning in cybersecurity environments, by modeling competing agency as part of the environment; in this way we allow the computation of optimal defensive strategies with single agent \gls{RL}.

Our research provides vital insights into building robust and trustworthy \gls{AI}, especially as the arms race between attackers and defenders continues to evolve.
The threat posed by adaptive adversaries, even when operating with minimal assumptions, and the vulnerabilities uncovered through our work, highlight a central theme -- achieving robust learning and inference requires first adaptive and optimized attacks, and active and adaptive defense mechanisms subsequently.
For example, in the case of reCAPTCHA v3, continuous behaviometric evaluation would require constant adaptations to counteract any automation that learns to evade detection.
With AutoRobust we demonstrate how the robustness of ML-based malware detection can be elevated from near zero to complete, by identifying and addressing exploitable features.
Lastly, through Adversarial Markov Games we introduce a methodology for optimizing and countering decision-based attacks, as well as what they entail: a landscape of ever-evolving adaptive adversaries.

Many researchers have noted that adversarial attacks, as they are typically conducted, largely remain academic exercises and fail to represent real-world scenarios~\cite{apruzzese2023real}.
However, our research consistently demonstrates that adaptive attackers pose a substantial threat to real-world AI-based systems, as the very act of deploying them inadvertently introduces new vulnerabilities.
Our findings are not merely based on a limited set of empirical results; we also conduct rigorous theoretical analyses which confirm our findings are no aberrations.
Consider that while antivirus solutions and their underlying models can be highly susceptible to adversarial attacks, this can be effectively remediated as we have demonstrated with AutoRobust.
This study underscores the importance of collaborative efforts between academia and industry to ensure the development and deployment of robust \gls{AI} systems, as adversarial threats will continue to proliferate.
Beyond the individual results, this dissertation lays a foundation for understanding and addressing these vulnerabilities by assembling insights from diverse domains, and setting the stage for creating more resilient and trustworthy learning and inference.

\section{The Next Steps}

Our work opens up several promising avenues for future research.
Beyond the diverse domains we have explored, it lays the groundwork for further investigations in multiple directions: (a) training more sophisticated agents with broader and more general capabilities, (b) enhancing and combining both stationary (hardening) and adaptive defense mechanisms (misdirection), and (c) extending research into domains where the problem-feature space gap has left problem-space adversarial attacks relatively underexplored.

AutoRobust can incorporate any other functionality-preserving modifications for generating adversarial malware in the problem space, that a growing body of related work is exploring~\cite{demetrio2021functionality, demetrio2021adversarial, labaca2021realizable, labaca2021aimed}.
Moreover, as we demonstrated in both the AutoRobust and AMG chapters, any combination of adversarial goals, be it performance, stealthiness, or disruption, can be optimized in a gradient-based manner, even in the complete black-box case and regardless of modality or domain.
Our approach is versatile and can be extended to various contexts where adversarial attacks are typically hindered by limited model access or the challenge of mapping perturbations back to the problem space.
Examples include \gls{LLM}s, autonomous vehicles, spam and network intrusion detection, and any other environment that contains observable agentic behavior.
The only requirement is that the adversary must have sufficient capability to influence the outcome; for instance, it would be futile if they can only manipulate the inter-packet interval while the \gls{NIDS} bases its classification on factors like geolocation or IP address.

At first glance, our work might seem to give adversaries an edge; however, this advantage exists only if these adversarial capabilities are not proactively employed to implement both passive and active defenses. For instance, with AutoRobust we reveal a significant and widespread vulnerability that arises when AI models are trained without accounting for adversarial agency: namely that they can be exceptionally vulnerable to adversarial examples and spurious correlations.
Yet, as we have demonstrated, these threats can be effectively mitigated by hardening the model using realistic adversaries.
In the specific context of malware detection through dynamic analysis reports, future research could focus on developing more fine-grained adversarial attacks that modify reports at the character level.
This can be achieved by leveraging modern LLMs, first to learn the underlying structure of these reports, and then to adapt the generation policy based on direct feedback from the classifier being targeted.

In our research, we have consistently modeled the opponent as part of the environment -- a necessary simplification that renders the environment stationary, thereby stabilizing policy learning.
Depending on the domain and the sophistication of the prevalent threats, future work could explore explicit rather than implicit opponent modeling, that allows agents to reason about each other recursively.
All of our studies are performed assuming the implicit case, where other agents' policies are treated as part of an environment with stationary dynamics.
This means that the learning agent models how the opponent behaves based on the observed history, but \emph{not} how the opponent \emph{would} behave based on how the agent behaves~\cite{albrecht2018autonomous, wen2019probabilistic}.
In inherently adversarial environments like cybersecurity, explicit opponent modelling opens a promising path towards more effective countermeasures.

In this dissertation we have shown how effective adversarial attacks can be, even without access to the underlying decision-making process.
Nevertheless, in an increasingly adversarial landscape, achieving robust and trustworthy \gls{AI} requires more than just resilience to attacks through adversarial training, adaptive defense mechanisms, and alignment.
Beyond AI security lies the realm of AI safety, where the focus expands from merely defending systems against adversaries to ensuring also that they do not function as adversaries themselves, and that their decisions align with ethical principles and societal values.
This naturally raises the question: whose principles and values should guide these systems?
At present, this depends largely on those who design these models and their willingness to adhere to regulatory frameworks.
The risks are further compounded by the limited explainability and consistency in interpreting decisions, even with \gls{XAI} techniques. 
This opacity hampers our understanding, making it difficult to anticipate, challenge, or correct erroneous and biased outcomes.

As researchers -- and citizens -- we must also reflect on our own reward mechanisms and how these may be misaligned or manipulated.
After all, the hypotheses we formulate and pursue spring from our collectively thinkable; the truth is still out there.
Since our data, skills, and the most essential aspects of our humanity are prerequisites for all the remarkable capabilities of contemporary \gls{AI}, a better approach to mitigating risks might not involve withholding this information from training.
Instead, it would be preferable to properly democratize the outcomes, by making the models and their capabilities openly accessible, thus ensuring their decision-making processes are transparent, accountable, and subject to public oversight.

\section{Beyond Security}

The accelerating progress in \gls{AI}, coupled with its swift adoption, have ushered in an era of significant potential benefits but also considerable risks.
\gls{LLM}s and foundation models showcase a wide range of impressive capabilities; however, they also come with numerous vulnerabilities.
These models are susceptible to prompt injection that can generate unsafe content, may provide incorrect, misleading, offensive, or biased information, and can even enable malicious activities such as generating malware code or crafting phishing emails.
Despite ongoing efforts to snuff out these issues, LLMs remain notably vulnerable.
As we outline in this dissertation, the maturity and reliability of defenses are often reflected in the sophistication and effectiveness of the attacks they withstand.
As of this writing, most attacks on LLMs rely on brute force and ad hoc methods, indicating that this research path holds great promise.
It also indicates that our understanding of the full scope and severity of these vulnerabilities is still far from complete.

Given our current understanding, what can we anticipate for the future of \gls{AI} safety?
The most widely used safety approach today is \gls{AI} alignment -- ensuring that a model's goals and learned behavior are consistent with human values and conform to ethical constraints.
Attempting to instill ethical behavior in models when they are still only capable of simulating and not actual reasoning, often clashes with the statistical correlations and biases present in their training data.
Even if we could overcome this challenge, aligning \gls{AI} with human values remains difficult due to the intrinsic limitations and fallibility of human reasoning itself.
This is known as the specification problem, the task of defining the precise goal and objectives that will lead to the desired outcomes~\cite{skalse2022defining}.
Approaches such as designing reward functions (as explored in this dissertation~\cite{silver2021reward}), learning from human behavior through \gls{IRL}, and ambitious value learning aim to capture the core of what we value, to then optimize it beyond human limitations.
As AI becomes more advanced it \textit{will} exhibit behavior that, just like deception in humans, appears to formally comply with the specification but deviates from the intended goal.

Alignment to human values faces several significant challenges.
First, there is no consensus on which these values should be.
Second, values are in flux and shaped by our environment.
Lastly, values themselves are often supplanted by reward or penalty schemes.
As a society, we still struggle to address long-term challenges effectively, as doing so requires cooperation in a highly decentralized and competitive world
The landscape of \gls{AI} safety is further complicated by the fluidity of human values and our inability to understand and interpret how complex systems interact at scale.
Consider how Google and OpenAI -- arguably the two most influential organizations in \gls{AI} research and development -- have evolved over time.
Google shifted from search engines and knowledge retrieval to advertising services, while OpenAI transitioned from a non-profit organization with an explicit mission to democratize \gls{AI} into a for-profit entity with closed-source models.
These shifts highlight the risk of AI systems becoming tools that exacerbate existing misalignments between economic incentives and societal goals.
The promise and hopeful outcome is that, down the line, \gls{AGI} will improve on our capabilities while aligning with our values.
A concerning possibility is that it will indeed outperform us, however on goals that are slightly to severely misaligned.


Recently, numerous researchers and prominent \gls{AI} pioneers\footnote{https://www.technologyreview.com/2023/05/03/1072589/video-geoffrey-hinton-google-ai-risk-ethics/} have expressed concerns about the existential risks posed by the very technology they help create.
While some of these risks might be overstated, others might be off the mark, at least in terms of urgency.
In popular imagination, the most striking depiction of misalignment is an \gls{AI} takeover leading to the subjugation of humanity.
In reality, such misalignments are \textit{already} evident in more subtle but significant ways -- namely, through the erosion of our collective agency and diminishing ability to steer ourselves and shape our environments.
This occurs as we increasingly rely on opaque, unaccountable \gls{AI} decision-making that remains obscure yet pervasive: from the information and news we receive to the targeted advertisements and online content that we consume.
In this dissertation, we have conducted a comprehensive investigation into methods for securing and ensuring the correct functioning of black-box \gls{AI} systems.
Beyond addressing the immediate failure modes however, as researchers we are compelled to investigate both the short- and long-term implications of integrating \gls{AI} into various aspects of life.
By applying alignment and other safety techniques, and through a critical reflection on the ways these systems influence us, we can work to \textit{secure} and \textit{ensure} that their impact remains positive and beneficial.

\section{Concluding Remarks}

In an era where \gls{AI} is becoming deeply embedded in both real-world applications and academic research, ensuring its security and proper functioning in the presence of adversaries is of paramount importance.
This widespread integration introduces new vulnerabilities and expands the attack surface for malicious actors, posing a particular threat to critical systems, such as those in cybersecurity.
In this dissertation we systematically assess the level of threat that adversaries pose, as a foundational step towards achieving robust and reliable AI-based systems.
We focus on three key domains: bot detection, malware detection, and image classification.
Through both theoretical and empirical investigations, we uncover the complexities and challenges inherent in each domain, providing valuable insights towards robust learning and inference.
Through our work, we provide a comprehensive approach for understanding and enhancing the resilience of AI models, emphasizing the critical role of adaptive strategies from both offensive and defensive perspectives.

In conclusion, the responsibility lies with both researchers and practitioners to understand these challenges and to design and deploy models that are resilient against real-world adversaries.
Towards this goal, our work lays a solid foundation for future research.
The methodology we developed -- optimizing all identified adversarial capabilities -- and is essential for accurately assessing the level of threat, can be applied to new domains.
Another important avenue for future research is evaluating how effectively passive defenses, like adversarial training, and adaptive ones can withstand novel and optimal threats, and if necessary, to devise \textit{new} defensive mechanisms.
In this perpetual game of cat and mouse, not all adversarial capabilities can be anticipated, yet through our work we contribute both a conceptual and practical framework for building resilient AI systems as new threats emerge. 
By deepening the understanding of black-box attacks and defenses, our research sets the stage as we scramble towards robust and trustworthy \gls{AI} systems deployed in the real world.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage