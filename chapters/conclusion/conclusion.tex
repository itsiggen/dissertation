% !TeX root = ../../thesis.tex
\chapter{Conclusion}\label{ch:conclusion}

\epigraph{"The novelist says in words what cannot be said in words."}{--- Ursula Le Guin}
% The artist deals in what cannot be said in words. The artist whose medium is fiction does this in words. 

The widespread and accelerating adoption of \gls{AI} across various domains is gradually transforming them, and in turn precipitates a fundamental shift on how we approach and interact with them. 
From cybersecurity, to finance, healthcare, media, and education, AI is becoming instrumental to the operation of many critical applications.
This in turn broadens the attack surface as novel threats and failure modes are introduced, like model extraction, model poisoning, and most prominently model evasion.
We should not however underestimate threats that lie beyond the model level.
While the fundamentals of modern AI have been around for a long time and survived two \gls{AI} winters, the latest progress has been fueled by advances in computation and the vast amounts of available data.
Notably, this availability of data is mostly due to the aggregate of our activity online, and are harvested mainly from the public domain, for instance Wikipedia and Stack Overflow\footnote{https://arstechnica.com/information-technology/2024/05/stack-overflow-users-sabotage-their-posts-after-openai-deal/}, to subsequently train models that become proprietary and closed-source.

While these advances are remarkable from multiple perspectives, like the potential leaps in productivity, the (so far false) promise of automation, and the small steps towards the holy grail of \gls{AGI}, they additionally raise serious questions on the security, safety, and accountability of \gls{AI} solutions, as well as on ethical and societal implications.
It is commonplace for \gls{AI} models to be trained and deployed with minimal consideration for their resilience against adversarial attacks.
Inherently adversarial contexts, like cybersecurity, exacerbate the generalization challenges these models already face by causing worst-case scenarios and distribution shifts, while also facing further threats beyond model evasion.
In this dissertation we contribute a methodological approach for investigating and evaluating the robustness of AI models which expose an interface to the real world, in a wide range of domains and modalities: from web automation and adversarial malware, to the primary \gls{AML} testbed, image classification.

Throughout our work, we arrive to several important takeaways and insights.
First, that as \gls{AI} capabilities keep improving, the logical conclusion in the competition between automation and its detection is that eventually the former will be favored, even when the detection mechanism and its internals are unknown.
Secondly, that the promise of generalization beyond the training set is often false and inaccurately represented by the performance on clean, non-adversarial test sets.
This underlines the importance of adversarial training, as a process of identification and removal of spurious correlations and brittle features that adversaries can exploit for evasion.
Finally, in certain domains model hardening defenses like adversarial training are insufficient as adaptive adversaries can bypass them; in such cases, active and adaptive defenses are also necessitated.
Resilience in cybersecurity is therefore achieved by fully adaptive and optimized attacks, a prerequisite for uncovering their full potential.
This is in order to find counterfactuals to harden the model with, as well as learn effective counters through adaptive defenses.

The ever-growing integration of \gls{AI} in the decision-making process of various systems introduces substantial security risks, particularly when these systems can be interfaced with.
A recurring principle in this dissertation is that having such an interface exposed, one that can be queried and interacted with, suffices for attackers to exploit the underlying model in various ways.
This exploitation extends further than manipulating the model decisions and correct functioning in general, to compromising the privacy of other users, and disrupting its availability and services.
However, as we gradually transition parts of our decision-making to AI, additional vulnerabilities are introduced where lapses in security and correct specification can have severe impact, from individuals and institutions to society at large. 
As the environments these AI models operate in form closed loops with their users, we do not only train and impart capabilities to these \gls{AI} models by generating data ourselves, we ourselves are also trained in turn.
That is to say in the future we will have to reckon not only with adversarial attacks against \gls{AI}, but also with the adversarial use of \gls{AI} against people, from something seemingly innocuous like engagement optimization to the brazenly antisocial like mass misinformation campaigns.

In this dissertation we have demonstrated how effective adversarial attacks can be even without access to the underlying decision-making process.
Nevertheless, in an increasingly adversarial landscape, for robust and trustworthy \gls{AI} it does not suffice to have models resilient to attacks through adversarial training, adaptive defense mechanisms, and alignment.
Beyond AI security lies AI safety, where the concern is not merely protecting these systems from adversaries, but with ensuring that they do not function as adversaries themselves and that their decisions align with ethical principles and societal values.
Naturally, one should ask whose principles and values precisely.
Currently, this means of those that create these models and their willingness to stay within regulatory frameworks.
The risks are further aggravated by the lack of explainability (with \gls{XAI}) and consistent interpretation of the decisions, something that obscures our understanding and makes difficult to foresee, challenge, or rectify erroneous and biased outcomes.

Finally, as researchers (and citizens) we also have to reflect on what our own reward mechanisms are and how these can be misaligned or manipulated.
After all, the hypotheses we conceive and pursue, spring from our collectively thinkable; the truth is still out there.
As our own data, skills, and what ultimately makes us human are prerequisites for all the extraordinary capabilities of contemporary \gls{AI}, perhaps then to mitigate risks instead of removing this information from training, it would be preferable to properly democratize the results: the models and their capabilities to be open, and their decision-making accountable and with complete public oversight.

\section{Summary of Contributions}
This dissertation is composed by a comprehensive examination of the inherent vulnerabilities and correct functioning of black-box, AI-based systems, in diverse applications and domains.
Specifically, we investigated: a) the most widely adopted CAPTCHA service that tells humans and bots apart, b) dynamic analysis based malware detection and how to harden it against evasion, and c) adaptive attacks, defenses, and the general competitive game they form in image classification.
In these three distinct yet interconnected cases, we reveal the complexities and idiosyncrasies of the interactions between offensive and defensive methods, in order to guarantee that the underlying models function according to their specification.

Our investigation into the Google reCAPTCHA v3 service highlighted its susceptibility to adversarial attacks, where the web browsing automation framework we developed reaches an evasion rate up to 99.6\%.
While bot detection that differentiates humans from bots through continuous behaviometric evaluation is and will remain an indispensable tool, our study highlights that it also inadvertently creates the conditions of its own obsolescence.
The risk scores generated are influenced by static aspects, for instance privacy settings and IP addresses, but also by the dynamic behavior of the user.
We demonstrate how the latter can be exploited, and develop \gls{RL} agents can simulate human-like browsing behavior that successfully evades detection.
Our findings underline the importance of developing more resilient CAPTCHA for enhanced bot and automation detection, potentially ones that move beyond current zero-friction, zero-challenge systems.

Our second study delves into the resilience of dynamic analysis based malware detection, specifically when ML-based models are employed against adversarial malware.
We introduce AutoRobust, a general methodology that leverages \gls{RL} to harden a model against evasion, and to identify and mitigate spurious correlations and brittle features.
We automate this demanding, human-in-the-loop process, by substituting the requirement for expert input on every potential shortcut feature, with asking a simple question: what are all the allowed ways something could be different and \textit{still} be the same thing?
Our approach imparts a probabilistic level of robustness against well-defined adversarial capabilities, thereby ensuring robust model performance \textit{without} compromising the efficacy on clean inputs.
Notably, our approach assumes nothing about the underlying model \textit{or} the specific problem-space capabilities that are used to generate adversarial malware, it is thus general and applicable to any use-case where gradients cannot be computed with respect to the inputs.
Our study illustrates that adversarial attacks on malware detection should be performed through the problem space, highlighting the importance of continuous adaptation and hardening of the detection models.

Our third study broadens our scope by examining decision-based attacks and defenses, as well as how both can adapt to each other.
First, we discover that adaptive attackers can perform exceedingly well, even against adversarially trained models; their effectiveness rests then on their ability to adapt the policies that govern their operation and their evasive arsenal \textit{in tandem}.
We introduce a framework named Adversarial Markov Games (AMG), which integrates, studies, and evaluates both theoretically and empirically decision-based attacks and defenses, first independently but also at their intersection.
We demonstrate that active defenses, which control how the system responds, are a necessary complement to model hardening when facing decision-based attacks; then how these defenses can be circumvented by adaptive attacks, only to finally elicit active and adaptive defenses.
Most importantly, through our AMG framework we simplify the complex task of learning in cybersecurity environments, by modeling competing agency as part of the environment; in this way we allow the computation of optimal defensive strategies with single agent \gls{RL}.

As a body of work, our research provides critical insights for robust and trustworthy \gls{AI} as the arms race between attackers and defenders is constantly evolving.
The threat that adaptive adversaries pose, even with minimal assumptions on their side, and the vulnerabilities exposed through our work, point to a common and central theme -- robust learning and inference can only be achieved through adaptive and optimized attacks first, and active and adaptive defense mechanisms subsequently.
In reCAPTCHA v3 for instance, continuous behaviometric evaluation would require constant adaptation to counteract any automation that learns to evade detection.
With AutoRobust we demonstrate that we can increase the robustness of ML-based malware detection systems from near zero to complete, by identifying and mitigating exploitable features.
Finally, our AMG framework provides a methodology for understanding and countering decision-based attacks, as well as what they entail: a landscape of ever-evolving adaptive adversaries.

Finally, a lot of researchers have pointed to the fact that adversarial attacks, at least the way the are performed so far, largely remain an academic exercise and do not reflect realistic scenarios~\cite{apruzzese2023real}.
Yet as we have consistently shown throughout or work, adaptive attackers pose a considerable threat against real-world AI-based systems, and the moment these systems are deployed serious vulnerabilities are inadvertently introduced.
Our evidence is not limited to a handful of empirical results, as we additionally perform a rigorous theoretical analysis which confirms our findings are no aberrations.
Nevertheless, while antivirus systems and their underlying models can be exceedingly vulnerable to adversaries, as we have demonstrated with AutoRobust, this can be effectively rectified.
Collaborative efforts between academia and industry will be crucial in ensuring the deployment of robust \gls{AI} systems as adversarial threats will continue to proliferate.
In this dissertation, besides the individual results we provide a foundation for understanding and addressing these vulnerabilities by assembling insights from diverse domains, and pave the way for developing more resilient and trustworthy learning and inference.

\section{The Next Steps}

Through our work we open multiple promising paths for future research.
Beyond the three concrete domains we investigated, our research opens several other lines of inquiry: a) training more sophisticated agents with expanded and more general capabilities, b) enhancing and combining both stationary (hardening) and adaptive (misdirection) defense mechanism, and c) expanding to other domains where, due to the problem-feature space gap, problem space adversarial attacks are relatively unexplored.

Specifically, AutoRobust can incorporate any other functionality-preserving capabilities for generating adversarial malware in the problem space, that a growing range of related work is exploring~\cite{demetrio2021functionality, demetrio2021adversarial, labaca2021realizable, labaca2021aimed}.
Furthermore, as we showed in both the AutoRobust and AMG chapters, any combination of adversarial goals, be it performance, stealthiness, or disruption, can be optimized in a gradient-based manner, even in the complete black-box case and in any modality.
This makes our approach easily extensible to a wide range of contexts where adversarial attacks are typically confounded by both lack of access to the model and the non-invertibility of perturbations back to the problem space.
Such examples include LLMs, autonomous vehicles, spam and network intrusion detection, and any other environment that contains observable agentic behavior.

Furthermore, with AutoRobust we bring to light a crucial and commonplace limitation when AI models are naively trained without respect to adversarial agency; namely that they can be exceptionally vulnerable to adversarial examples and spurious correlations.
As we have demonstrated however, it is very feasible to mitigate these threats by hardening the model with realistic adversaries.
In dynamic analysis report based malware detection specifically, future work can explore more powerful modification policies that are able to adjust reports at the character level.
This can be done by leveraging modern LLMs to learn the intrinsic structure of reports and subsequently adapt the generation policy based on direct feedback from the classifier under attack.

Finally, through our research we have always modeled the opponent as part of the environment; this is often a necessary simplification that renders the environment stationary and in that way stabilizes the learning of policies.
Depending on the domain and the sophistication of its prevalent threats, future studies can incorporate explicit opponent modeling and recursive reasoning where agents hold beliefs about each other.
In the study of opponent modeling, considering other agent policies as stationary and part of the environment is equivalent to \textit{0th} level recursive reasoning:
the agent models how the opponent behaves based on the observed history, but \emph{not} how the opponent \emph{would} behave based on how the agent behaves~\cite{albrecht2018autonomous, wen2019probabilistic}.
Intrinsically adversarial environments like cybersecurity, show the highest potential in benefiting from explicit opponent modelling.

\section{Beyond Security}

The accelerating progress in \gls{AI} together with its quick adoption has ushered in an era where the potential benefits and risks are both considerable.
LLMs and foundation models in general exhibit a multitude of impressive capabilities, however they also arrive with a host of issues: they are susceptible to prompt injection to produce unsafe content, can provide incorrect, misleading, offensive, or biased information, and can facilitate malicious behavior like writing malware code or phishing emails.
Despite continuous efforts to snuff out their failure modes, LLMs remain remarkably vulnerable.
As we trace out in this dissertation, a good indication of the level of maturity and reliability of defenses is the level of sophistication and optimality of the employed attacks.
At the time of writing, attacks against LLMs employ predominantly brute force and ad hoc techniques, indicating that it is a very promising and unexplored field, and that we do not have yet a comprehensive understanding on the severity and range of vulnerabilities.

So given what we currently know, what can we extrapolate that lies down the road for \gls{AI} safety?
The most widely employed safety technique currently is \gls{AI} alignment, the process of ensuring that the goals and learned behavior are consistent with human values and adhere to ethical constraints.
Attempting to instill ethical behavior to models when they still cannot perform actual reasoning but so far only simulate it, often conflicts with the statistical correlations and biases present in their training data.
Even if that would be solved however, the task of aligning \gls{AI} to human values is confounded by the inherent limitations and fallibility of human reasoning itself.
This is known as the specification problem, the task of defining the correct goal and objectives that will lead to the desirable outcome.
Designing reward functions (like we do in this dissertation), learning from human behavior through \gls{IRL}, and ambitious value learning, are some approaches to that end, which aim to capture the essence of what we as humans care about to be subsequently optimized beyond our performance limitations.
Just like humans however, AI systems can exhibit behavior that while formally compliant with the specification it deviates from the intended goals. 

Alignment to human values faces multiple challenges.
First, there is no consensus on which these values are.
Secondly, values are in flux and shaped by our environment.
Finally, values themselves are often replaced by reward or penalty schemes.
We are still not very good at solving our own long term problems as this requires cooperation in a vastly decentralized and competitive world.
The safety landscape of \gls{AI} is further complicated by the malleability of human values and our inability to understand and interpret how systems interact at scale.
Consider how Google and OpenAI have changed as entities over time, arguably the two most prominent organizations for \gls{AI} research and development today.
Google shifted from search engines and knowledge retrieval to ads services; OpenAI transitioned from a non-profit company with an explicit mission to democratize \gls{AI} to a for-profit with closed-source models.
This tendency underscores the risk of AI systems becoming tools that perpetuate and amplify misalignments already existing between economic incentives and societal goals.
The promise and hopeful outcome is that, down the line, \gls{AGI} greatly improves on our capabilities while conforming to our values; a conceivable outcome is that it will outperform us indeed, but on goals that are slightly to catastrophically misaligned.

In conclusion, while the existential risks from AI might be exaggerated, they might also be completely off the mark, at least in terms of urgency.
In our collective imagination the most prominent misalignment example is an \gls{AI} takeover and the subjugation or annihilation of humanity.
In practice, such misalignments already manifest in the loss of collective agency and ability to steer ourselves and our environments when we gradually transition towards black-box, unaccountable \gls{AI} decision-making that remains obscure yet pervasive: from the books and news articles to the ads and online content that we are recommended.
In this dissertation we provided a comprehensive investigation of how to secure and ensure the correct functioning of black-box \gls{AI} systems.
Besides the immediate failure modes however, as researchers we are compelled to investigate the short and long term consequences of integrating \gls{AI} into various facets of life.
By using alignment techniques and through a critical reflection on how these systems affect us, we can also secure and ensure that their influence and impact will be beneficial.

\section{Concluding Remarks}

In an era where \gls{AI} is becoming integral to both real-world applications and academic research, the importance of its security and proper functioning in the presence of adversaries cannot be overstated.
This integration introduces new vulnerabilities and expands the attack surface for malicious actors, especially for critical systems. 
In this dissertation we systematically examine the level of threat that AI-enabled adversaries pose, as a prerequisite towards the robust and reliable functioning of AI-based systems.

We focus on three distinct domains, bot detection, malware detection, and image classification, where our theoretical and empirical investigations reveal the complexities and challenges therein and provide crucial insights towards robust learning and inference.
First, we discover that adaptive attackers pose a considerable threat against any decision-making system that exposes an interface that can be queried.
Secondly, this threat can be partially mitigated if the above process is performed as part of adversarially training the model.
Thirdly, for occasions where adversarial training will be insufficient, active and adaptive defenses that reject or misdirect adversaries have a complementary potential.
Finally, our Adversarial Markov Games (AMG) framework delves into the interplay and merges adaptive attacks and defenses.
Through it, we provide a nuanced approach for understanding and enhancing the resilience of AI models, highlighting the necessity of adaptive strategies from both offensive and defensive perspectives.
In conclusion, with this dissertation we advance the technical understanding of black-box \gls{AML} and set the stage for future research towards robust and trustworthy \gls{AI} systems deployed in the real world.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage